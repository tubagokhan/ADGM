{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "mount_file_id": "1ISWTw4ZltFJPPfgEJDMjD2rqK_6b9_vI",
      "authorship_tag": "ABX9TyOzlV00z1LNAlo5WjusHt3h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tubagokhan/ADGM/blob/main/TextClassificationWithoutDistinctionWithLemma.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHJc0RsaekF_",
        "outputId": "13a478c1-997f-47bb-906c-c2972e5efc8b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu118)\n",
            "Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.24.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHIXoTS-b8gN",
        "outputId": "7b0bd888-b7df-45c4-ef3c-6d6d1c095300"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSgRaJMcec5z",
        "outputId": "a1e60920-d4a2-465f-f037-3bb34013328c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique classes: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Epoch 1/50, Loss: 1.6251: 100%|██████████| 209/209 [00:39<00:00,  5.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Average Loss: 1.6250700625506314\n",
            "Validation Accuracy (Epoch 1): 0.5321\n",
            "Epoch 1 - F1 Score: 0.5320729237002025\n",
            "Saved model checkpoint with validation accuracy: 0.5321\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/50, Loss: 1.0302: 100%|██████████| 209/209 [00:39<00:00,  5.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - Average Loss: 1.0301906857193943\n",
            "Validation Accuracy (Epoch 2): 0.5901\n",
            "Epoch 2 - F1 Score: 0.5901417960837272\n",
            "Saved model checkpoint with validation accuracy: 0.5901\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/50, Loss: 0.8330: 100%|██████████| 209/209 [00:39<00:00,  5.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - Average Loss: 0.8330413522332479\n",
            "Validation Accuracy (Epoch 3): 0.5949\n",
            "Epoch 3 - F1 Score: 0.5948683322079676\n",
            "Saved model checkpoint with validation accuracy: 0.5949\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/50, Loss: 0.7219: 100%|██████████| 209/209 [00:39<00:00,  5.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 - Average Loss: 0.7218837404365175\n",
            "Validation Accuracy (Epoch 4): 0.6144\n",
            "Epoch 4 - F1 Score: 0.6144496961512491\n",
            "Saved model checkpoint with validation accuracy: 0.6144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/50, Loss: 0.6475: 100%|██████████| 209/209 [00:39<00:00,  5.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 - Average Loss: 0.6474618602312353\n",
            "Validation Accuracy (Epoch 5): 0.6205\n",
            "Epoch 5 - F1 Score: 0.6205266711681297\n",
            "Saved model checkpoint with validation accuracy: 0.6205\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/50, Loss: 0.6056: 100%|██████████| 209/209 [00:39<00:00,  5.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 - Average Loss: 0.605626612473903\n",
            "Validation Accuracy (Epoch 6): 0.6158\n",
            "Epoch 6 - F1 Score: 0.6158001350438893\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/50, Loss: 0.5814: 100%|██████████| 209/209 [00:39<00:00,  5.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 - Average Loss: 0.5814020284340142\n",
            "Validation Accuracy (Epoch 7): 0.6111\n",
            "Epoch 7 - F1 Score: 0.6110735989196489\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/50, Loss: 0.5715: 100%|██████████| 209/209 [00:39<00:00,  5.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 - Average Loss: 0.5715060583427192\n",
            "Validation Accuracy (Epoch 8): 0.6172\n",
            "Epoch 8 - F1 Score: 0.6171505739365294\n",
            "Early stopping triggered. No improvement for 3 epochs.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load your JSON data\n",
        "with open('/content/drive/Othercomputers/MBZUAI/MBZUAI/Codes/COBS_All_Labels_PhraseswithLemma.json', 'r', encoding='utf-8-sig') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Extract labels and phrases\n",
        "labels = [item[\"label_\"] for item in data]\n",
        "phrases = [item[\"phrase\"] for item in data]\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    phrases, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Further split the training data into training and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Load BERT tokenizer and encode the data\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "train_encodings = tokenizer(train_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "val_encodings = tokenizer(val_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "test_encodings = tokenizer(test_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Encode labels using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
        "val_labels_encoded = label_encoder.transform(val_labels)\n",
        "test_labels_encoded = label_encoder.transform(test_labels)\n",
        "\n",
        "unique_labels = set(labels)\n",
        "num_classes = len(unique_labels)\n",
        "print(f\"Number of unique classes: {num_classes}\")\n",
        "\n",
        "# Create BERT-based text classification model\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_classes)\n",
        "\n",
        "# Move the model and data to the GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "train_encodings = {key: value.to(device) for key, value in train_encodings.items()}\n",
        "val_encodings = {key: value.to(device) for key, value in val_encodings.items()}\n",
        "test_encodings = {key: value.to(device) for key, value in test_encodings.items()}\n",
        "train_labels_encoded = torch.tensor(train_labels_encoded, device=device)\n",
        "val_labels_encoded = torch.tensor(val_labels_encoded, device=device)\n",
        "test_labels_encoded = torch.tensor(test_labels_encoded, device=device)\n",
        "\n",
        "# Fine-tuning hyperparameters\n",
        "batch_size = 64\n",
        "max_seq_length = 128\n",
        "learning_rate = 2e-5\n",
        "num_epochs = 50\n",
        "gradient_accumulation_steps = 4\n",
        "\n",
        "# Create data loaders\n",
        "train_dataset = TensorDataset(train_encodings[\"input_ids\"], train_encodings[\"attention_mask\"], train_labels_encoded)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Optimization and loss function\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 3\n",
        "best_val_accuracy = 0\n",
        "no_improvement_count = 0\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    tqdm_data = tqdm(enumerate(train_loader), total=len(train_loader))\n",
        "    for step, batch in tqdm_data:\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        inputs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
        "        outputs = model(**inputs)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        if (step + 1) % gradient_accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        avg_loss = total_loss / (step + 1)\n",
        "\n",
        "        tqdm_data.set_description(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "    print(f\"Epoch {epoch + 1} - Average Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "    # Evaluation on validation data after each epoch\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(**val_encodings)\n",
        "        val_predicted_labels = torch.argmax(val_outputs.logits, dim=1)\n",
        "        val_accuracy = accuracy_score(val_labels_encoded.cpu().numpy(), val_predicted_labels.cpu().numpy())\n",
        "        print(f\"Validation Accuracy (Epoch {epoch+1}): {val_accuracy:.4f}\")\n",
        "        f1 = f1_score(val_labels_encoded.cpu().numpy(), val_predicted_labels.cpu().numpy(), average='micro')\n",
        "        print(f\"Epoch {epoch + 1} - F1 Score: {f1}\")\n",
        "\n",
        "        # Implement early stopping\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            no_improvement_count = 0\n",
        "\n",
        "            # Save the model checkpoint with the best validation accuracy\n",
        "            model_checkpoint_path = '/content/drive/Othercomputers/MBZUAI/MBZUAI/Codes/best_model.pt'\n",
        "            torch.save(model.state_dict(), model_checkpoint_path)\n",
        "            print(\"Saved model checkpoint with validation accuracy: {:.4f}\".format(val_accuracy))\n",
        "        else:\n",
        "            no_improvement_count += 1\n",
        "            if no_improvement_count >= patience:\n",
        "                print(\"Early stopping triggered. No improvement for {} epochs.\".format(patience))\n",
        "                break  # Stop training\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model for evaluation on the GPU\n",
        "loaded_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_classes)\n",
        "loaded_model.load_state_dict(torch.load(model_checkpoint_path))\n",
        "loaded_model.to(device)  # Move the loaded model to the GPU\n",
        "loaded_model.eval()  # Set the loaded model in evaluation mode for inference\n",
        "\n",
        "# Evaluation on test data\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_outputs = loaded_model(**test_encodings)  # Use loaded_model for inference\n",
        "    test_predicted_labels = torch.argmax(test_outputs.logits, dim=1)\n",
        "    test_accuracy = accuracy_score(test_labels_encoded.cpu().numpy(), test_predicted_labels.cpu().numpy())\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "    f1 = f1_score(test_labels_encoded.cpu().numpy(), test_predicted_labels.cpu().numpy(), average='micro')\n",
        "    print(\"Micro F1-Score:\", f1)\n",
        "\n",
        "    test_report = classification_report(test_labels_encoded.cpu().numpy(), test_predicted_labels.cpu().numpy())\n",
        "    print(\"Test Classification Report:\\n\", test_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FncD_cFv9NWJ",
        "outputId": "03d6bc8a-7f29-4048-923b-81834c10bc4e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.6189\n",
            "Micro F1-Score: 0.6188546731496488\n",
            "Test Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.71      0.66       270\n",
            "           1       0.55      0.82      0.66      1135\n",
            "           2       0.77      0.12      0.20       598\n",
            "           3       0.75      0.67      0.70       269\n",
            "           4       0.79      0.66      0.72       662\n",
            "           5       0.00      0.00      0.00        18\n",
            "           6       0.65      0.65      0.65        48\n",
            "           7       0.58      0.68      0.62       373\n",
            "           8       0.58      0.60      0.59       329\n",
            "\n",
            "    accuracy                           0.62      3702\n",
            "   macro avg       0.59      0.54      0.53      3702\n",
            "weighted avg       0.65      0.62      0.59      3702\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_labels\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2vyXdvkf3fT",
        "outputId": "76e15fe6-a4ce-4f39-e816-811a0b765e18"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ACT', 'DEF', 'ENT', 'FS', 'MIT', 'PERM', 'PROD', 'RISK', 'TECH'}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import numpy as np\n",
        "\n",
        "# Load the new JSON file containing phrases\n",
        "with open('/content/drive/Othercomputers/MBZUAI/MBZUAI/Codes/samplePhrasesandTags100.json', 'r', encoding='utf-8-sig') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "# Extract phrases and original labels from the new data\n",
        "phrases = [item[\"phrase\"] for item in data]\n",
        "original_labels = [item[\"label_\"] for item in data]\n",
        "\n",
        "# Preprocess the new data\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "encoded_phrases = tokenizer(phrases, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Move the new data to the GPU\n",
        "encoded_phrases = {key: value.to(device) for key, value in encoded_phrases.items()}\n",
        "\n",
        "# Use the pre-trained model to predict labels\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    outputs = model(**encoded_phrases)\n",
        "    predicted_labels = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "# Define your custom label encoder for the missing label 'PERM'\n",
        "class CustomLabelEncoder:\n",
        "    def transform(self, labels):\n",
        "        return [label if label != 'PERM' else '8' for label in labels]\n",
        "\n",
        "# Create an instance of your custom label encoder\n",
        "label_encoder = CustomLabelEncoder()\n",
        "\n",
        "# Decode the predicted labels\n",
        "predicted_labels_decoded = label_encoder.transform(predicted_labels)\n",
        "\n",
        "# Define a reverse label mapping\n",
        "reverse_label_mapping = {\n",
        "    '8': 'PERM',\n",
        "    '0': 'ACT',  # Replace '0' with the corresponding label\n",
        "    '1': 'DEF',\n",
        "    '2': 'ENT',\n",
        "    '3': 'FS',\n",
        "    '4': 'MIT',\n",
        "    '5': 'PROD',\n",
        "    '6': 'RISK',\n",
        "    '7': 'TECH',# Replace '1' with the corresponding label\n",
        "    # Add mappings for other labels as needed\n",
        "}\n",
        "\n",
        "# Convert numeric labels (tensors) to text labels\n",
        "predicted_labels_text = [reverse_label_mapping[str(label.item())] for label in predicted_labels]\n",
        "\n",
        "# Print the original phrases and their predicted labels\n",
        "for phrase, original_label, predicted_label, predicted_label_value in zip(phrases, original_labels, predicted_labels_text, predicted_labels.tolist()):\n",
        "    print(f\"Phrase: {phrase}\")\n",
        "    print(f\"KG Label: {original_label}\")\n",
        "    print(f\"Predicted Label: {predicted_label}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1kHqG_eBmTK",
        "outputId": "0514d980-bafd-4b69-897b-54382f1a79b5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phrase: indefinite\n",
            "KG Label: FS\n",
            "Predicted Label: TECH\n",
            "\n",
            "Phrase: Executive/CE\n",
            "KG Label: MIT\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: Illustrative Regulatory Framework\n",
            "KG Label: TECH\n",
            "Predicted Label: TECH\n",
            "\n",
            "Phrase: might take in the medium term (i.e. five to ten years\n",
            "KG Label: MIT\n",
            "Predicted Label: TECH\n",
            "\n",
            "Phrase: We propose to include a section in the concept paper that describes a future\n",
            "KG Label: MIT\n",
            "Predicted Label: MIT\n",
            "\n",
            "Phrase: an example of a DeFi insurance protocol\n",
            "KG Label: RISK\n",
            "Predicted Label: MIT\n",
            "\n",
            "Phrase: except that this tends to focus on risks specific to DeFi such as the risk of a hack or of a failure in a smart contract\n",
            "KG Label: MIT\n",
            "Predicted Label: TECH\n",
            "\n",
            "Phrase: insurance\n",
            "KG Label: FS\n",
            "Predicted Label: FS\n",
            "\n",
            "Phrase: hack\n",
            "KG Label: ACT\n",
            "Predicted Label: PERM\n",
            "\n",
            "Phrase: market\n",
            "KG Label: FS\n",
            "Predicted Label: FS\n",
            "\n",
            "Phrase: regulators\n",
            "KG Label: DEF\n",
            "Predicted Label: ENT\n",
            "\n",
            "Phrase: regime from the industry and other regulators to test\n",
            "KG Label: MIT\n",
            "Predicted Label: MIT\n",
            "\n",
            "Phrase: We would invite comment\n",
            "KG Label: MIT\n",
            "Predicted Label: MIT\n",
            "\n",
            "Phrase: might look like\n",
            "KG Label: RISK\n",
            "Predicted Label: MIT\n",
            "\n",
            "Phrase: would make clear that the regulatory framework is only meant to illustrate what key features\n",
            "KG Label: RISK\n",
            "Predicted Label: TECH\n",
            "\n",
            "Phrase: would not be bound to implement such a regime\n",
            "KG Label: RISK\n",
            "Predicted Label: TECH\n",
            "\n",
            "Phrase: abstract on the industry and regulators own\n",
            "KG Label: RISK\n",
            "Predicted Label: MIT\n",
            "\n",
            "Phrase: future state and high level policy\n",
            "KG Label: RISK\n",
            "Predicted Label: MIT\n",
            "\n",
            "Phrase: such an illustrative framework would be to provide a more\n",
            "KG Label: MIT\n",
            "Predicted Label: TECH\n",
            "\n",
            "Phrase: The objective of including\n",
            "KG Label: RISK\n",
            "Predicted Label: TECH\n",
            "\n",
            "Phrase: of what a regulatory regime based on the above high-level policy\n",
            "KG Label: RISK\n",
            "Predicted Label: MIT\n",
            "\n",
            "Phrase: We propose to set out key features\n",
            "KG Label: MIT\n",
            "Predicted Label: MIT\n",
            "\n",
            "Phrase: positions\n",
            "KG Label: TECH\n",
            "Predicted Label: FS\n",
            "\n",
            "Phrase: ties together\n",
            "KG Label: TECH\n",
            "Predicted Label: TECH\n",
            "\n",
            "Phrase: viability\n",
            "KG Label: FS\n",
            "Predicted Label: TECH\n",
            "\n",
            "Phrase: regulators\n",
            "KG Label: FS\n",
            "Predicted Label: ENT\n",
            "\n",
            "Phrase: invite\n",
            "KG Label: FS\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: look\n",
            "KG Label: FS\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: positions\n",
            "KG Label: FS\n",
            "Predicted Label: FS\n",
            "\n",
            "Phrase: it is likely that DeFi will continue to grow\n",
            "KG Label: MIT\n",
            "Predicted Label: TECH\n",
            "\n",
            "Phrase: estimated at US$1.7tn and 81m respectively)\n",
            "KG Label: RISK\n",
            "Predicted Label: TECH\n",
            "\n",
            "Phrase: cryptocurrency space\n",
            "KG Label: MIT\n",
            "Predicted Label: PERM\n",
            "\n",
            "Phrase: addresses 2 participating in DeFi grew from about 90,000 addresses to about 4.2 million addresses\n",
            "KG Label: RISK\n",
            "Predicted Label: MIT\n",
            "\n",
            "Phrase: the total number of unique\n",
            "KG Label: MIT\n",
            "Predicted Label: MIT\n",
            "\n",
            "Phrase: Over the same period\n",
            "KG Label: MIT\n",
            "Predicted Label: MIT\n",
            "\n",
            "Phrase: nearly 380 times from about US$630 million to about US$238 billion\n",
            "KG Label: RISK\n",
            "Predicted Label: TECH\n",
            "\n",
            "Phrase: ”) 1 in DeFi protocols grew by\n",
            "KG Label: MIT\n",
            "Predicted Label: TECH\n",
            "\n",
            "Phrase: the total value locked\n",
            "KG Label: MIT\n",
            "Predicted Label: MIT\n",
            "\n",
            "Phrase: Between 1 Jan 2020 and 31 Dec 2021,\n",
            "KG Label: MIT\n",
            "Predicted Label: MIT\n",
            "\n",
            "Phrase: over the past two years\n",
            "KG Label: RISK\n",
            "Predicted Label: TECH\n",
            "\n",
            "Phrase: strong growth in both value and users\n",
            "KG Label: MIT\n",
            "Predicted Label: TECH\n",
            "\n",
            "Phrase: DeFi has had\n",
            "KG Label: MIT\n",
            "Predicted Label: MIT\n",
            "\n",
            "Phrase: nearly\n",
            "KG Label: TECH\n",
            "Predicted Label: MIT\n",
            "\n",
            "Phrase: capitalisation\n",
            "KG Label: FS\n",
            "Predicted Label: FS\n",
            "\n",
            "Phrase: locked\n",
            "KG Label: ACT\n",
            "Predicted Label: MIT\n",
            "\n",
            "Phrase: total value\n",
            "KG Label: FS\n",
            "Predicted Label: FS\n",
            "\n",
            "Phrase: value\n",
            "KG Label: FS\n",
            "Predicted Label: FS\n",
            "\n",
            "Phrase: persons\n",
            "KG Label: DEF\n",
            "Predicted Label: ENT\n",
            "\n",
            "Phrase: based on\n",
            "KG Label: RISK\n",
            "Predicted Label: TECH\n",
            "\n",
            "Phrase: of\n",
            "KG Label: RISK\n",
            "Predicted Label: MIT\n",
            "\n",
            "Phrase: on the governance model\n",
            "KG Label: RISK\n",
            "Predicted Label: MIT\n",
            "\n",
            "Phrase: whether directly or indirectly\n",
            "KG Label: MIT\n",
            "Predicted Label: TECH\n",
            "\n",
            "Phrase: the key principle would be to identify persons who can exercise significant control\n",
            "KG Label: RISK\n",
            "Predicted Label: TECH\n",
            "\n",
            "Phrase: potential governance approaches\n",
            "KG Label: RISK\n",
            "Predicted Label: TECH\n",
            "\n",
            "Phrase: Identifying the\n",
            "KG Label: MIT\n",
            "Predicted Label: MIT\n",
            "\n",
            "Phrase: exercise\n",
            "KG Label: ACT\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: these positions\n",
            "KG Label: MIT\n",
            "Predicted Label: MIT\n",
            "\n",
            "Phrase: concept paper setting out\n",
            "KG Label: MIT\n",
            "Predicted Label: MIT\n",
            "\n",
            "Phrase: approval for the\n",
            "KG Label: RISK\n",
            "Predicted Label: MIT\n",
            "\n",
            "Phrase: To seek\n",
            "KG Label: MIT\n",
            "Predicted Label: MIT\n",
            "\n",
            "Phrase: Committee’s\n",
            "KG Label: ENT\n",
            "Predicted Label: MIT\n",
            "\n",
            "Phrase: assets\n",
            "KG Label: DEF\n",
            "Predicted Label: FS\n",
            "\n",
            "Phrase: asset\n",
            "KG Label: DEF\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: securities\n",
            "KG Label: DEF\n",
            "Predicted Label: FS\n",
            "\n",
            "Phrase: in return for a yield\n",
            "KG Label: MIT\n",
            "Predicted Label: TECH\n",
            "\n",
            "Phrase: who will lock away their assets\n",
            "KG Label: RISK\n",
            "Predicted Label: MIT\n",
            "\n",
            "Phrase: provided by the users of\n",
            "KG Label: MIT\n",
            "Predicted Label: TECH\n",
            "\n",
            "Phrase: based on the ratio of available\n",
            "KG Label: RISK\n",
            "Predicted Label: TECH\n",
            "\n",
            "Phrase: the price of\n",
            "KG Label: MIT\n",
            "Predicted Label: TECH\n",
            "\n",
            "Phrase: securities/derivatives\n",
            "KG Label: MIT\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: DEXes\n",
            "KG Label: MIT\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: some decentralized exchanges\n",
            "KG Label: MIT\n",
            "Predicted Label: TECH\n",
            "\n",
            "Phrase: assets\n",
            "KG Label: FS\n",
            "Predicted Label: FS\n",
            "\n",
            "Phrase: liquidity\n",
            "KG Label: FS\n",
            "Predicted Label: FS\n",
            "\n",
            "Phrase: orders\n",
            "KG Label: ACT\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: derivatives\n",
            "KG Label: PROD\n",
            "Predicted Label: RISK\n",
            "\n",
            "Phrase: securities\n",
            "KG Label: PROD\n",
            "Predicted Label: FS\n",
            "\n",
            "Phrase: book\n",
            "KG Label: FS\n",
            "Predicted Label: FS\n",
            "\n",
            "Phrase: order\n",
            "KG Label: FS\n",
            "Predicted Label: FS\n",
            "\n",
            "Phrase: and obtain a return\n",
            "KG Label: MIT\n",
            "Predicted Label: MIT\n",
            "\n",
            "Phrase: the underlying reasons why people consume financial services remains the same\n",
            "KG Label: MIT\n",
            "Predicted Label: TECH\n",
            "\n",
            "Phrase: TradFi, in much the same way that financial services\n",
            "KG Label: MIT\n",
            "Predicted Label: PERM\n",
            "\n",
            "Phrase: how people consume financial services\n",
            "KG Label: MIT\n",
            "Predicted Label: TECH\n",
            "\n",
            "Phrase: it may significantly change\n",
            "KG Label: RISK\n",
            "Predicted Label: TECH\n",
            "\n",
            "Phrase: see DeFi\n",
            "KG Label: MIT\n",
            "Predicted Label: MIT\n",
            "\n",
            "Phrase: we propose to\n",
            "KG Label: MIT\n",
            "Predicted Label: MIT\n",
            "\n",
            "Phrase: As a\n",
            "KG Label: MIT\n",
            "Predicted Label: TECH\n",
            "\n",
            "Phrase: payments\n",
            "KG Label: TECH\n",
            "Predicted Label: PERM\n",
            "\n",
            "Phrase: electronic trading\n",
            "KG Label: TECH\n",
            "Predicted Label: PERM\n",
            "\n",
            "Phrase: trading\n",
            "KG Label: TECH\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: payments\n",
            "KG Label: ACT\n",
            "Predicted Label: PERM\n",
            "\n",
            "Phrase: services\n",
            "KG Label: ACT\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: trading\n",
            "KG Label: ACT\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: very\n",
            "KG Label: FS\n",
            "Predicted Label: MIT\n",
            "\n",
            "Phrase: position\n",
            "KG Label: FS\n",
            "Predicted Label: FS\n",
            "\n",
            "Phrase: Utility tokens are not virtual\n",
            "KG Label: RISK\n",
            "Predicted Label: TECH\n",
            "\n",
            "Phrase: the operation of the DeFi protocol\n",
            "KG Label: RISK\n",
            "Predicted Label: MIT\n",
            "\n",
            "Phrase: and burnt\n",
            "KG Label: MIT\n",
            "Predicted Label: TECH\n",
            "\n",
            "Phrase: used by the DeFi protocol\n",
            "KG Label: MIT\n",
            "Predicted Label: MIT\n",
            "\n",
            "Phrase: • DeFi utility token\n",
            "KG Label: MIT\n",
            "Predicted Label: TECH\n",
            "\n"
          ]
        }
      ]
    }
  ]
}