{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm",
      "mount_file_id": "1-FRB1AnrGLIirpm0atleOkNuH2QTjwrO",
      "authorship_tag": "ABX9TyM9W1ttyzY2/ByUzA2BT7dm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tubagokhan/ADGM/blob/main/MultilabelClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "fdgmqmCo0fKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rXJpbXojNskT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Define the label types\n",
        "label_types = [\"PERM\", \"DEF\", \"RISK\", \"MIT\", \"ENT\", \"ACT\", \"FS\", \"PROD\", \"TECH\"]\n",
        "\n",
        "# Load your JSON data\n",
        "with open('/content/drive/Othercomputers/MBZUAI/MBZUAI/Codes/records.json', 'r' , encoding='utf-8-sig') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "# Prepare your dataset\n",
        "texts = []  # Store text data\n",
        "labels = []  # Store binary-encoded labels\n",
        "\n",
        "# Convert label data into binary format\n",
        "for item in data:\n",
        "    text = item[\"Paragraph\"]\n",
        "    text = text.lower()  # Convert text to lowercase\n",
        "    tags = item[\"Tags\"]\n",
        "    label = [0] * len(label_types)\n",
        "\n",
        "    for tag in tags:\n",
        "        tag_type = tag[\"Type\"]\n",
        "        if tag_type in label_types:\n",
        "            label[label_types.index(tag_type)] = 1\n",
        "\n",
        "    texts.append(text)\n",
        "    labels.append(label)\n",
        "\n",
        "# Split the dataset into training and testing\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize the text data\n",
        "max_seq_length = 128  # Adjust as needed\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding='max_length', max_length=max_seq_length, return_tensors='pt', return_attention_mask=True)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding='max_length', max_length=max_seq_length, return_tensors='pt', return_attention_mask=True)\n",
        "\n",
        "# Create PyTorch Dataset\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item['labels'] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = CustomDataset(train_encodings, torch.tensor(train_labels, dtype=torch.float32))\n",
        "test_dataset = CustomDataset(test_encodings, torch.tensor(test_labels, dtype=torch.float32))\n",
        "\n",
        "# Now you can use train_dataset and test_dataset to train and evaluate your BERT-based multi-label classification model.\n"
      ],
      "metadata": {
        "id": "BXnO3TOlAoxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import BertForSequenceClassification, AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "from tqdm import tqdm  # Import tqdm for the progress bar\n",
        "\n",
        "# Load BERT model\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_types))\n",
        "\n",
        "# Define hyperparameters\n",
        "batch_size = 64  # Change the batch size\n",
        "learning_rate = 2e-5\n",
        "num_epochs = 50  # Increase the number of epochs\n",
        "early_stop_patience = 3  # Number of epochs to wait for improvement before early stopping\n",
        "\n",
        "# Initialize optimizer and loss function\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "best_f1_score = 0.0\n",
        "early_stop_counter = 0\n",
        "\n",
        "# Training loop with early stopping and progress bar\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
        "    for i, batch in progress_bar:\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "        labels = batch[\"labels\"]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix({'Loss': total_loss / (i + 1)})  # Update the progress bar\n",
        "\n",
        "    print(f\"Epoch {epoch + 1} - Average Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "    # Evaluate the model and calculate F1 score on the validation set\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch[\"input_ids\"]\n",
        "            attention_mask = batch[\"attention_mask\"]\n",
        "            labels = batch[\"labels\"]\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            predictions.extend(torch.sigmoid(logits).cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    f1 = f1_score(np.array(true_labels), np.array(predictions) > 0.5, average='micro')\n",
        "\n",
        "    print(f\"Epoch {epoch + 1} - F1 Score: {f1}\")\n",
        "\n",
        "    # Check for early stopping\n",
        "    if f1 > best_f1_score:\n",
        "        best_f1_score = f1\n",
        "        early_stop_counter = 0\n",
        "    else:\n",
        "        early_stop_counter += 1\n",
        "        if early_stop_counter >= early_stop_patience:\n",
        "            print(f\"Early stopping at epoch {epoch + 1} as F1 score did not improve.\")\n",
        "            break\n"
      ],
      "metadata": {
        "id": "ZAjCHKnjWoOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "predicted_labels = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "        labels = batch[\"labels\"]\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        predicted_labels.extend(torch.sigmoid(logits).cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "predicted_labels = np.array(predicted_labels)\n",
        "true_labels = np.array(true_labels)\n",
        "\n",
        "# Compute F1-score for each label\n",
        "f1_scores = f1_score(true_labels, (predicted_labels > 0.5), average='micro')\n",
        "print(\"Micro F1-Score:\", f1_scores)\n"
      ],
      "metadata": {
        "id": "vxKT2lt3Ca2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Input sentence\n",
        "sentence = \"the aggregate of each of the following in, or relating to, the Clients portfolio at the close of business on the valuation date.\"\n",
        "# Tokenize the sentence\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "# Perform part-of-speech tagging\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "# Create a grammar for phrase structure parsing\n",
        "grammar = r\"\"\"\n",
        "    NP: {<DT>?<JJ>*<NN>}\n",
        "    PP: {<IN><NP>}\n",
        "    VP: {<VB.*><NP|PP|CLAUSE>+$}\n",
        "    CLAUSE: {<NP><VP>}\n",
        "\"\"\"\n",
        "chunk_parser = nltk.RegexpParser(grammar)\n",
        "\n",
        "# Parse the sentence\n",
        "tree = chunk_parser.parse(pos_tags)\n",
        "\n",
        "# Initialize a list to store the phrases\n",
        "phrases = []\n",
        "\n",
        "# Define a function to extract phrases from the tree\n",
        "def extract_phrases(t):\n",
        "    if isinstance(t, nltk.Tree):\n",
        "        phrase = \" \".join([word for word, tag in t.leaves()])\n",
        "        phrases.append(phrase)\n",
        "        for subtree in t:\n",
        "            extract_phrases(subtree)\n",
        "\n",
        "# Extract phrases from the tree\n",
        "extract_phrases(tree)\n",
        "\n",
        "# Print the extracted phrases\n",
        "#for phrase in phrases:\n",
        "#    print(phrase)\n"
      ],
      "metadata": {
        "id": "m2BZagSzQ0mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "for phrase in phrases:\n",
        "  # Tokenize the input sentence\n",
        "  #input_sentence = \"the aggregate of each of the following in, or relating to, the Clients portfolio at the close of business on the valuation date.\"\n",
        "  tokenized_input = tokenizer(phrase, truncation=True, padding='max_length', max_length=max_seq_length, return_tensors='pt', return_attention_mask=True)\n",
        "\n",
        "  # Pass the tokenized input through the model\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      output = model(**tokenized_input)\n",
        "\n",
        "  # Interpret the model's output\n",
        "  logits = output.logits\n",
        "  predicted_labels = torch.sigmoid(logits).cpu().numpy()[0]  # Assuming you have a single input sentence\n",
        "\n",
        "  # Identify the predicted phrases and labels\n",
        "  predicted_phrases_and_labels = []\n",
        "  for i, label_type in enumerate(label_types):\n",
        "      if predicted_labels[i] > 0.5:\n",
        "          predicted_phrases_and_labels.append({\n",
        "              \"Type\": label_type,\n",
        "              \"Phrase\": phrase  # In this example, we assume the entire sentence is the phrase\n",
        "          })\n",
        "\n",
        "  # Display the predicted phrases and labels\n",
        "  print(\"Predicted Phrases and Labels:\")\n",
        "  for item in predicted_phrases_and_labels:\n",
        "      print(f\"Type: {item['Type']}, Phrase: {item['Phrase']}\")\n",
        "  print(\" \")\n"
      ],
      "metadata": {
        "id": "C4ydL8KxFg4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the extracted phrases\n",
        "for phrase in phrases:\n",
        "    print(phrase)"
      ],
      "metadata": {
        "id": "jEjTB8yHtoMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Read the JSON file\n",
        "with open('/content/drive/Othercomputers/MBZUAI/MBZUAI/Codes/samplePhrasesandTags100.json', 'r', encoding='utf-8-sig') as json_file:\n",
        "    sample_data = json.load(json_file)\n",
        "\n",
        "# Extract values from the JSON data and populate the lists\n",
        "SamplePhrases = [data['phrase'] for data in sample_data]\n",
        "SampleTags = [data['label_'] for data in sample_data]\n",
        "\n",
        "\n",
        "for t in range(len(SamplePhrases)):\n",
        "  print((\"KG: \" +SampleTags[t]))\n",
        "  # Tokenize the input sentence\n",
        "  #input_sentence = \"the aggregate of each of the following in, or relating to, the Clients portfolio at the close of business on the valuation date.\"\n",
        "  tokenized_input = tokenizer(SamplePhrases[t], truncation=True, padding='max_length', max_length=max_seq_length, return_tensors='pt', return_attention_mask=True)\n",
        "\n",
        "  # Pass the tokenized input through the model\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      output = model(**tokenized_input)\n",
        "\n",
        "  # Interpret the model's output\n",
        "  logits = output.logits\n",
        "  predicted_labels = torch.sigmoid(logits).cpu().numpy()[0]\n",
        "\n",
        "\n",
        "    # Find the index of the label type with the highest prediction\n",
        "  highest_label_index = np.argmax(predicted_labels)\n",
        "\n",
        "  # Save the related data for the highest label\n",
        "  predicted_phrases_and_labels = [{\n",
        "      \"Type\": label_types[highest_label_index],\n",
        "      \"Phrase\": SamplePhrases[t]\n",
        "  }]\n",
        "\n",
        "  # Display the predicted phrases and labels\n",
        "  prediction = predicted_phrases_and_labels[0]\n",
        "  print(f\"Type: {prediction['Type']}, Phrase: {prediction['Phrase']}\")\n",
        "  print(\" \")\n"
      ],
      "metadata": {
        "id": "BJXN7pbQzojn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}