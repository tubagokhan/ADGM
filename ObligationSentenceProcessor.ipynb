{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPq5B9s7LZ4XeXiYlyykJpo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tubagokhan/ADGM/blob/main/ObligationSentenceProcessor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-aWGgo_RZLE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "from openai import AzureOpenAI\n",
        "\n",
        "# Configure API settings via environment variables\n",
        "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
        "os.environ[\"OPENAI_API_VERSION\"] = \"2024-02-15-preview\"\n",
        "\n",
        "# Setup Azure OpenAI client\n",
        "client = AzureOpenAI(\n",
        "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
        "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
        "    api_version=os.getenv(\"OPENAI_API_VERSION\")\n",
        ")\n",
        "\n",
        "def verify_label(sentence, label):\n",
        "    prompt = f\"Does the following sentence represent an {label}? '{sentence}' Provide a simple 'yes' or 'no'.\"\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": prompt},\n",
        "                {\"role\": \"user\", \"content\": \"\"}\n",
        "            ],\n",
        "            model=\"gpt-4-turbo-1106\",\n",
        "            temperature=0.0,\n",
        "            max_tokens=8,\n",
        "            top_p=1.0,\n",
        "            frequency_penalty=0,\n",
        "            presence_penalty=0,\n",
        "            stop=[\"\\n\"]\n",
        "        )\n",
        "        return response.choices[0].message.content.strip().lower() == 'yes'\n",
        "    except Exception as e:\n",
        "        print(f\"Error verifying sentence label: {e}\")\n",
        "        return False\n",
        "\n",
        "def generate_sentences(text, label, iterations):\n",
        "    all_sentences = []\n",
        "    for i in range(iterations):\n",
        "        print(f\"Generating {label} sentences, iteration {i + 1}...\")\n",
        "        prompt = f\"\"\"\n",
        "        Context: Review the document thoroughly. Identify sections that specify guidelines but do not impose obligations.\n",
        "\n",
        "        Task: Generate {label} sentences based on the document content.\n",
        "\n",
        "        Content: {text}\n",
        "        \"\"\"\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": prompt},\n",
        "                    {\"role\": \"user\", \"content\": \"\"}\n",
        "                ],\n",
        "                model=\"gpt-4-turbo-1106\",\n",
        "                temperature=0.7,\n",
        "                max_tokens=1024,\n",
        "                top_p=0.95,\n",
        "                frequency_penalty=0,\n",
        "                presence_penalty=0,\n",
        "                stop=None\n",
        "            )\n",
        "            sentences = response.choices[0].message.content.strip().split(\"\\n\")\n",
        "            verified_sentences = [{\"Text\": sentence.strip(), \"Obligation\": label == 'obligation'} for sentence in sentences if sentence.strip()]\n",
        "            all_sentences.extend(verified_sentences)\n",
        "            print(f\"Added {len(verified_sentences)} verified {label} sentences.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating sentences: {e}\")\n",
        "\n",
        "    return all_sentences\n",
        "\n",
        "def process_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    obligations = generate_sentences(text, \"obligation\", 5)\n",
        "    non_obligations = generate_sentences(text, \"non-obligation\", 5)\n",
        "\n",
        "    json_data = obligations + non_obligations\n",
        "    json_file_path = os.path.splitext(file_path)[0] + \"_sentences.json\"\n",
        "\n",
        "    with open(json_file_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(json_data, f, indent=4)\n",
        "    print(f\"Sentences saved successfully to {json_file_path}\")\n",
        "\n",
        "def combine_and_shuffle_json_files(directory_path, output_file):\n",
        "    all_data = []\n",
        "    obligation_count = 0\n",
        "    non_obligation_count = 0\n",
        "\n",
        "    for filename in os.listdir(directory_path):\n",
        "        if filename.endswith(\"_sentences.json\"):\n",
        "            file_path = os.path.join(directory_path, filename)\n",
        "            print(f\"Reading data from {file_path}...\")\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                data = json.load(file)\n",
        "                all_data.extend(data)\n",
        "                for item in data:\n",
        "                    if item[\"Obligation\"]:\n",
        "                        obligation_count += 1\n",
        "                    else:\n",
        "                        non_obligation_count += 1\n",
        "\n",
        "    print(\"Shuffling data...\")\n",
        "    random.shuffle(all_data)\n",
        "\n",
        "    output_path = os.path.join(directory_path, output_file)\n",
        "    with open(output_path, 'w', encoding='utf-8') as file:\n",
        "        json.dump(all_data, file, indent=4)\n",
        "\n",
        "    print(f\"Total items: {len(all_data)}\")\n",
        "    print(f\"Obligation items: {obligation_count}\")\n",
        "    print(f\"Non-obligation items: {non_obligation_count}\")\n",
        "    print(f\"Shuffled data saved to {output_path}\")\n",
        "\n",
        "# Processing all files in a directory\n",
        "directory_path = \"your_directory_here\"\n",
        "for filename in os.listdir(directory_path):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        process_file(os.path.join(directory_path, filename))\n",
        "\n",
        "# Combine and shuffle all created JSON files\n",
        "output_file = \"ObligationNonObligationSentenceDataset.json\"\n",
        "combine_and_shuffle_json_files(directory_path, output_file)\n"
      ]
    }
  ]
}