{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "mount_file_id": "1ISWTw4ZltFJPPfgEJDMjD2rqK_6b9_vI",
      "authorship_tag": "ABX9TyNL8Ji8+WjvGdnnGadPi8wM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tubagokhan/ADGM/blob/main/TextClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHJc0RsaekF_",
        "outputId": "4253edbb-d41e-404f-8a86-0caa12313f7c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu118)\n",
            "Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.24.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHIXoTS-b8gN",
        "outputId": "117f323d-825f-4449-fb1d-92c194ac2616"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSgRaJMcec5z",
        "outputId": "b0571c0b-e888-4bb2-8d85-60096ec85960"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique classes: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Epoch 1/50, Loss: 1.8957: 100%|██████████| 57/57 [09:12<00:00,  9.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Average Loss: 1.8957092448284751\n",
            "Validation Accuracy (Epoch 1): 0.4557\n",
            "Epoch 1 - F1 Score: 0.45566502463054187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/50, Loss: 1.4532: 100%|██████████| 57/57 [09:08<00:00,  9.63s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - Average Loss: 1.4531835443095158\n",
            "Validation Accuracy (Epoch 2): 0.4532\n",
            "Epoch 2 - F1 Score: 0.4532019704433497\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/50, Loss: 1.2296: 100%|██████████| 57/57 [09:03<00:00,  9.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - Average Loss: 1.2295860501757838\n",
            "Validation Accuracy (Epoch 3): 0.5493\n",
            "Epoch 3 - F1 Score: 0.5492610837438424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/50, Loss: 1.0718: 100%|██████████| 57/57 [09:09<00:00,  9.64s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 - Average Loss: 1.071755559820878\n",
            "Validation Accuracy (Epoch 4): 0.5961\n",
            "Epoch 4 - F1 Score: 0.5960591133004927\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/50, Loss: 0.9218: 100%|██████████| 57/57 [09:12<00:00,  9.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 - Average Loss: 0.9218076948533979\n",
            "Validation Accuracy (Epoch 5): 0.6182\n",
            "Epoch 5 - F1 Score: 0.6182266009852216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/50, Loss: 0.7680: 100%|██████████| 57/57 [09:10<00:00,  9.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 - Average Loss: 0.7680167487838812\n",
            "Validation Accuracy (Epoch 6): 0.5862\n",
            "Epoch 6 - F1 Score: 0.5862068965517241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/50, Loss: 0.6277: 100%|██████████| 57/57 [09:04<00:00,  9.55s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 - Average Loss: 0.6277279137519368\n",
            "Validation Accuracy (Epoch 7): 0.5961\n",
            "Epoch 7 - F1 Score: 0.5960591133004927\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/50, Loss: 0.5234: 100%|██████████| 57/57 [09:07<00:00,  9.60s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 - Average Loss: 0.5234113628404182\n",
            "Validation Accuracy (Epoch 8): 0.6059\n",
            "Epoch 8 - F1 Score: 0.6059113300492611\n",
            "Early stopping triggered. No improvement for 3 epochs.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load your JSON data\n",
        "with open('/content/drive/Othercomputers/MBZUAI/MBZUAI/Codes/COBS_All_Labels_Phrases_Distinct.json', 'r', encoding='utf-8-sig') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Extract labels and phrases\n",
        "labels = [item[\"label_\"] for item in data]\n",
        "phrases = [item[\"phrase\"] for item in data]\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    phrases, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Further split the training data into training and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Load BERT tokenizer and encode the data\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "train_encodings = tokenizer(train_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "val_encodings = tokenizer(val_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "test_encodings = tokenizer(test_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Encode labels using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
        "val_labels_encoded = label_encoder.transform(val_labels)\n",
        "test_labels_encoded = label_encoder.transform(test_labels)\n",
        "\n",
        "unique_labels = set(labels)\n",
        "num_classes = len(unique_labels)\n",
        "print(f\"Number of unique classes: {num_classes}\")\n",
        "\n",
        "# Create BERT-based text classification model\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_classes)\n",
        "\n",
        "# Fine-tuning hyperparameters\n",
        "batch_size = 64\n",
        "max_seq_length = 128\n",
        "learning_rate = 2e-5\n",
        "num_epochs = 50  # Increase the number of epochs for early stopping\n",
        "gradient_accumulation_steps = 4\n",
        "\n",
        "# Create data loaders\n",
        "train_dataset = TensorDataset(train_encodings[\"input_ids\"], train_encodings[\"attention_mask\"], torch.tensor(train_labels_encoded))\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Optimization and loss function\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 3  # Number of epochs to wait for improvement\n",
        "best_val_accuracy = 0\n",
        "no_improvement_count = 0\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    tqdm_data = tqdm(enumerate(train_loader), total=len(train_loader))\n",
        "    for step, batch in tqdm_data:\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        if (step + 1) % gradient_accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        avg_loss = total_loss / (step + 1)\n",
        "\n",
        "        tqdm_data.set_description(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "    print(f\"Epoch {epoch + 1} - Average Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "\n",
        "    # Evaluation on validation data after each epoch\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(**val_encodings)\n",
        "        val_predicted_labels = torch.argmax(val_outputs.logits, dim=1)\n",
        "        val_accuracy = accuracy_score(val_labels_encoded, val_predicted_labels)\n",
        "        print(f\"Validation Accuracy (Epoch {epoch+1}): {val_accuracy:.4f}\")\n",
        "        f1 = f1_score(val_labels_encoded, val_predicted_labels,average='micro')\n",
        "        print(f\"Epoch {epoch + 1} - F1 Score: {f1}\")\n",
        "\n",
        "        # Implement early stopping\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            no_improvement_count = 0\n",
        "        else:\n",
        "            no_improvement_count += 1\n",
        "            if no_improvement_count >= patience:\n",
        "                print(\"Early stopping triggered. No improvement for {} epochs.\".format(patience))\n",
        "                break  # Stop training\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on test data\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(**test_encodings)\n",
        "    test_predicted_labels = torch.argmax(test_outputs.logits, dim=1)\n",
        "    test_accuracy = accuracy_score(test_labels_encoded, test_predicted_labels)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "    f1 = f1_score(test_labels_encoded, test_predicted_labels,average='micro')\n",
        "    print(\"Micro F1-Score:\", f1)\n",
        "\n",
        "\n",
        "    '''\n",
        "    test_report = classification_report(test_labels_encoded, test_predicted_labels)\n",
        "    print(\"Test Classification Report:\\n\", test_report)'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2vyXdvkf3fT",
        "outputId": "23f1baa5-2761-44f5-b8fa-cad48d35e6b3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.5953\n",
            "Micro F1-Score: 0.5952615992102666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the new JSON file containing phrases\n",
        "with open('/content/drive/Othercomputers/MBZUAI/MBZUAI/Codes/samplePhrasesandTags100.json', 'r', encoding='utf-8-sig') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "# Extract phrases and original labels from the new data\n",
        "phrases = [item[\"phrase\"] for item in data]\n",
        "original_labels = [item[\"label_\"] for item in data]\n",
        "\n",
        "# Load the pre-trained model and tokenizer\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Tokenize and encode the phrases\n",
        "encodings = tokenizer(phrases, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Load the label encoder (you should have it from your previous code)\n",
        "# Fit the label encoder with the original labels\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(original_labels)  # Fit the label encoder with the original labels\n",
        "\n",
        "# Use the model to predict labels for the phrases\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(**encodings)\n",
        "    predicted_labels = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "# Decode the predicted labels\n",
        "decoded_labels = label_encoder.inverse_transform(predicted_labels)\n",
        "\n",
        "# Print the original labels, predicted labels, and predicted label values for each phrase\n",
        "for phrase, original_label, predicted_label, predicted_label_value in zip(phrases, original_labels, decoded_labels, predicted_labels.tolist()):\n",
        "    print(f\"Phrase: {phrase}\")\n",
        "    print(f\"KG Label: {original_label}\")\n",
        "    print(f\"Predicted Label: {predicted_label}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elYWfhPoiDvu",
        "outputId": "a6a03bf9-fc05-4423-ad3d-bdf62347d3ab"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phrase: indefinite\n",
            "KG Label: FS\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: Executive/CE\n",
            "KG Label: MIT\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: Illustrative Regulatory Framework\n",
            "KG Label: TECH\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: might take in the medium term (i.e. five to ten years\n",
            "KG Label: MIT\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: We propose to include a section in the concept paper that describes a future\n",
            "KG Label: MIT\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: an example of a DeFi insurance protocol\n",
            "KG Label: RISK\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: except that this tends to focus on risks specific to DeFi such as the risk of a hack or of a failure in a smart contract\n",
            "KG Label: MIT\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: insurance\n",
            "KG Label: FS\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: hack\n",
            "KG Label: ACT\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: market\n",
            "KG Label: FS\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: regulators\n",
            "KG Label: DEF\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: regime from the industry and other regulators to test\n",
            "KG Label: MIT\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: We would invite comment\n",
            "KG Label: MIT\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: might look like\n",
            "KG Label: RISK\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: would make clear that the regulatory framework is only meant to illustrate what key features\n",
            "KG Label: RISK\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: would not be bound to implement such a regime\n",
            "KG Label: RISK\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: abstract on the industry and regulators own\n",
            "KG Label: RISK\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: future state and high level policy\n",
            "KG Label: RISK\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: such an illustrative framework would be to provide a more\n",
            "KG Label: MIT\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: The objective of including\n",
            "KG Label: RISK\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: of what a regulatory regime based on the above high-level policy\n",
            "KG Label: RISK\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: We propose to set out key features\n",
            "KG Label: MIT\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: positions\n",
            "KG Label: TECH\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: ties together\n",
            "KG Label: TECH\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: viability\n",
            "KG Label: FS\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: regulators\n",
            "KG Label: FS\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: invite\n",
            "KG Label: FS\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: look\n",
            "KG Label: FS\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: positions\n",
            "KG Label: FS\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: it is likely that DeFi will continue to grow\n",
            "KG Label: MIT\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: estimated at US$1.7tn and 81m respectively)\n",
            "KG Label: RISK\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: cryptocurrency space\n",
            "KG Label: MIT\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: addresses 2 participating in DeFi grew from about 90,000 addresses to about 4.2 million addresses\n",
            "KG Label: RISK\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: the total number of unique\n",
            "KG Label: MIT\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: Over the same period\n",
            "KG Label: MIT\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: nearly 380 times from about US$630 million to about US$238 billion\n",
            "KG Label: RISK\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: ”) 1 in DeFi protocols grew by\n",
            "KG Label: MIT\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: the total value locked\n",
            "KG Label: MIT\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: Between 1 Jan 2020 and 31 Dec 2021,\n",
            "KG Label: MIT\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: over the past two years\n",
            "KG Label: RISK\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: strong growth in both value and users\n",
            "KG Label: MIT\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: DeFi has had\n",
            "KG Label: MIT\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: nearly\n",
            "KG Label: TECH\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: capitalisation\n",
            "KG Label: FS\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: locked\n",
            "KG Label: ACT\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: total value\n",
            "KG Label: FS\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: value\n",
            "KG Label: FS\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: persons\n",
            "KG Label: DEF\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: based on\n",
            "KG Label: RISK\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: of\n",
            "KG Label: RISK\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: on the governance model\n",
            "KG Label: RISK\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: whether directly or indirectly\n",
            "KG Label: MIT\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: the key principle would be to identify persons who can exercise significant control\n",
            "KG Label: RISK\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: potential governance approaches\n",
            "KG Label: RISK\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: Identifying the\n",
            "KG Label: MIT\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: exercise\n",
            "KG Label: ACT\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: these positions\n",
            "KG Label: MIT\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: concept paper setting out\n",
            "KG Label: MIT\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: approval for the\n",
            "KG Label: RISK\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: To seek\n",
            "KG Label: MIT\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: Committee’s\n",
            "KG Label: ENT\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: assets\n",
            "KG Label: DEF\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: asset\n",
            "KG Label: DEF\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: securities\n",
            "KG Label: DEF\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: in return for a yield\n",
            "KG Label: MIT\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: who will lock away their assets\n",
            "KG Label: RISK\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: provided by the users of\n",
            "KG Label: MIT\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: based on the ratio of available\n",
            "KG Label: RISK\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: the price of\n",
            "KG Label: MIT\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: securities/derivatives\n",
            "KG Label: MIT\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: DEXes\n",
            "KG Label: MIT\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: some decentralized exchanges\n",
            "KG Label: MIT\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: assets\n",
            "KG Label: FS\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: liquidity\n",
            "KG Label: FS\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: orders\n",
            "KG Label: ACT\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: derivatives\n",
            "KG Label: PROD\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: securities\n",
            "KG Label: PROD\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: book\n",
            "KG Label: FS\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: order\n",
            "KG Label: FS\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: and obtain a return\n",
            "KG Label: MIT\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: the underlying reasons why people consume financial services remains the same\n",
            "KG Label: MIT\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: TradFi, in much the same way that financial services\n",
            "KG Label: MIT\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: how people consume financial services\n",
            "KG Label: MIT\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: it may significantly change\n",
            "KG Label: RISK\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: see DeFi\n",
            "KG Label: MIT\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: we propose to\n",
            "KG Label: MIT\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: As a\n",
            "KG Label: MIT\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: payments\n",
            "KG Label: TECH\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: electronic trading\n",
            "KG Label: TECH\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: trading\n",
            "KG Label: TECH\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: payments\n",
            "KG Label: ACT\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: services\n",
            "KG Label: ACT\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: trading\n",
            "KG Label: ACT\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: very\n",
            "KG Label: FS\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: position\n",
            "KG Label: FS\n",
            "Predicted Label: ACT\n",
            "\n",
            "Phrase: Utility tokens are not virtual\n",
            "KG Label: RISK\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: the operation of the DeFi protocol\n",
            "KG Label: RISK\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: and burnt\n",
            "KG Label: MIT\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: used by the DeFi protocol\n",
            "KG Label: MIT\n",
            "Predicted Label: DEF\n",
            "\n",
            "Phrase: • DeFi utility token\n",
            "KG Label: MIT\n",
            "Predicted Label: DEF\n",
            "\n"
          ]
        }
      ]
    }
  ]
}